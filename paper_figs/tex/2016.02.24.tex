\documentclass{article}
\title{Data-mining sloppiness: Look ma, no parameters!}

\usepackage{
amsmath, amssymb, amsfonts, chemarr,
xcolor, epsf, float, subcaption
}

\usepackage{graphicx}
\graphicspath{ {../figs/} }

\setlength{\parindent}{0pt}

\newcommand{\N}{\mathrm{N}}
\newcommand{\T}{\mathrm{T}}
\newcommand{\X}{\mathrm{X}}
\newcommand{\D}{\mathrm{D}}
\newcommand{\V}{\mathcal{V}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\ip}[2]{\langle#1,#2\rangle}
\newcommand{\abs}[1]{\vert#1\vert}
\newcommand{\norm}[1]{\vert\vert#1\vert\vert}
\newcommand{\dv}[2]{d(#1,#2)}
\newcommand{\eps}{\varepsilon}
\newcommand{\ex}{{\rm e}}
\newcommand{\ssep}{:}

\newcommand{\p}{\theta}
\newcommand{\fmr}{\chi}
\newcommand{\fmm}{\mathcal{X}}
\newcommand{\fiber}{\mathcal{F}}
\newcommand{\omr}{\mu}
\newcommand{\omm}{\mathcal{M}}
\newcommand{\R}{\mathrm{R}}
\newcommand{\ps}{\mathrm{\Theta}}
\newcommand{\fms}{\mathrm{X}}

\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\dst}{\displaystyle}

\newcommand{\Ord}{\mathcal{O}}

\author{}

\begin{document}
\maketitle

\section{Introduction}

In a 2006 computational neuroscience paper, after repeatedly fitting a ``reasonable" mechanistic neuron model to experimental firing data, the authors arrived at a multitude of ``good enough" parameter values - values that gave acceptable agreement between model prediction and experiments in the comparison norm chosen. They then had the insightful idea to try and ``summarize" this set of ``good enough" parameter values through principal component analysis - in effect, they passed hyperplanes through their sampling of the "good enough" subset of their parameter space (good enough
for their experimental data and with their goodness criterion).
%
In effect, they created -through data mining- a parametrization of what we might call the ``zero level set" of their objective/fitness function (again, for the experimental data available and the comparison norm chosen).

Clearly, the fact that the dimensionality of this ``good set" (for short) was more than zero implied that there are
directions in parameter space (in their work, local linear combinations of the model parameters) that keep the
predictions satisfactory.
%
This then implies that the number of directions in model parameter space that actually make a difference in the predictions is less than the full parameter space dimension.
%
The model has therefore less ``effective parameters" than \em{model parameters}.

There is hardly any novelty in this observation - there have been many efforts (and in recent years notably the program on "model/parameter sloppiness" from J. Sethna's group at Cornell, continued and extended by his former student
N. Transtrum and coworkers, or the ``active subspaces" program of P. Constantine).
%
What was interesting for us in this paper, however, was the attempt to provide a quantitative description of an important aspect of sloppiness (the parametrization of the zero level set) \em{through data mining}.

In this paper, motivated by the above (and not only) we will attempt to start building a
systematic link between \em{nonlinear} data mining and ``sloppiness".
%
Our research background is in model reduction of complex (many-degree-of-freedom) dynamical systems;
our experience is therefore much more on state-variable reduction
(simplifying modeling by deriving reduced dimensionality models) - rather than in \em{parameter reduction}.
%
The context might be best illustrated by thinking of reducing a time-dependent
flow in a complex geometry - one goes from a ``huge" discretization of the Navier-Stokes equations to a
low-dimensional Galerkin projection in terms of empirical basis functions;
yet both the original problem and the "state-reduced" problem have \em{one and the same} important parameter:
the Reynolds number.
%
In contrast,when reducing a large chemical (e.g. combustion) or biochemical (e.g.metabolic) reaction network, it is clear
that one desires a model that is not only reduced in terms of effective number of variables,
but also reduced in terms of effective parameters - parameters that would arise in a
reduced dynamic model that, one expects, are (possibly linear, but in general nonlinear) functions of
combinations the (full, detailed) model parameters.
%
For several years we have been developing a framework for "on-the-fly", data based reduction approach to
modeling complex systems (what we call the equation-free approach);
we attempt to solve reduced models that are unavailable in closed form by
judicious short bursts of computation with an (assumed accurate) detailed model.
%
There is a natural link between this approach and data mining: these brief bursts
of simulation of the high dimensional detailed model produce rich, high dimensional
data-sets; mining these data (through manifold learning techniques, for our group often through diffusion maps)
suggests the reduced set of \em{effective state variables} in terms of which the reduced model should be locally cast.
%
This we call the \em{variable free} component of our approach: the reduced model variables are
determined not through experience or through mathematics, but rather dictated through data mining of model simulations.
%
Clearly, it would be naive to embark on such a process if one is capable of deriving closed form equations in
terms of explicit reduced variables determined through mathematics (e.g. a few leading moments of particle distributions,
a few "important" concentrations, a few empirical --PCA-based-- orthogonal basis function coefficients).
%
Yet, -also clearly- if one cannot succeed in such a derivation, this does not mean that one must
remain condemned to lengthy fine scale simulations and verbal/statistical descriptions of their results.
%
Knowing that an effective (reduced) equation in terms of some effective (reduced) observables/variables is
possible, can still lead to the efficient extraction of information from the fine scale model.

Our ambition is to extend this approach beyond the \em{data-driven} determination of "the right effective
variables", to the data-driven determination of "the right effective parameters".
%
This way, not only dynamical simulations can be accelerated, but also
parametric explorations and optimization computations ("dynamics" in parameter space!) can be
enhanced.


A few words before starting:

We already stated that we want to develop data-driven ways to accomplish the detection of 
\em{effective} (important, relevant) parameters or parameter combinations within a large "model parameter" set.
%
Sensitivity analysis (in particular, computer-assisted sensitivity analysis) is one of the ways this is
currently approached in many disciplines; yet it is interesting to consider the way we do this analytically 
(when possible). 
%
The appropriate context is, of course, perturbation theory: typically, one would discard a term in an expansion
if the term is relatively small; the coefficient of this term is then a sloppy parameter (any value that
keeps the term relatively small does not alter the predictions of the model). 
%
Such a coefficient may well be
a function of several model parameters (ratios of sums of kinetic constants, for example). 
%
In the context of \em{time-dependent models}, we know that there are two ``types" of perturbations:
regular, and singular. 
%
The coefficients of \em{regular} perturbation terms can be ignored (they are then ``sloppy"), and the
parameter set can thus be reduced, while the number of effective variables remains the same.
%
In the case of \em{singular} perturbations,  as illustrated by the ``standard" two-variable singularly
perturbed ODE system
......


where a sufficiently small coefficient multiplies the highest
time derivative, the sloppiness of this coefficient (and the concomitant reduction of model parameters) 
is inextricably linked to the reduction of the dimensionality of the long-term system response.
%
How do we then test numerically for ``regular" vs. ``singular" sloppiness (the latter
also linked to potential long-time model reduction) ? 
%
And while the long-term dynamics may reduce, if one has accurate measurements of initial transients, 
this could ``lift" sloppiness in both state-space and parameter-space reduction; the point here being
that it is the time scales of interest to the observer (and, therefore, the type of measurements the
observer chooses or is able to perform) that may very well affect potential model and parameter reduction.

We believe it is worth considering (in the context of the above illustrative caricature) the inclusion of the
initial condition $y_0 \equiv y(t=0)$ in the ``fast" variable $y$ as a parameter, affecting the system response, that is
to be inferred from later measurements.
%
Clearly, if one only wants to fit long-time measurements (long compared to the fast time scale $eps$) there exists a multitude of ``good enough" initial conditions (a one-parameter family, lying on the relevant iscochrone) that fit the bill.
%
This infinity (and therefore also ``sloppiness") of ``equivalent" initial conditions in \em{state space}), which is the
essential manifestation of state-space model reducibility, naturally then becomes part of \em{parameter reduction};
there is an apparent duality between sloppiness in the singular perturbation parameter $\eps$ and in our
ability to derive a long-term accurate reduced dynamic model ignoring the fast foliation of state space in the 
$y$ direction: singular parameter perturbation and model reduction are inextricably linked.
%

One can also draw a different \em{conceptual} analogy between ``dynamic state reduction" and ``parameter reduction" in the
context of optimization. 
%
Indeed, an optimization algorithm (from conjugate gradients to simulated annealing)
endows parameter space with ``search dynamics" - each iteration of the algorithm is ``time 1" in this
optimization clock. 
%
In the same sense that fast equilibrating variables can be ignored in long-term dynamics (remember again
the illustrative caricature we discussed above), sensitive parameter directions quickly ``fathomed" can
be also factored out in the ``slow", long-term optimization steps in the less-sensitive (and thus
``optimization slow") directions.
%
%Remarkably, in optimization it is the ``slow parameters", that matter less, that we are interested in 
%exploring. 
%
%In dynamics, it is the parameters who ``matter more" that are the effective parameters we want to discover.

One of the issues we will attempt to demonstrate through an \em{ad hoc} designed caricature, is that a
simple, generic parameter optimization problem, with a simple quadratic minimum and no parameter sloppiness
can -through a ``bad enough" transformation- be turned into a sloppy one.
%
This is not surprising, but it also suggests the converse: that sloppy problems are possibly only sloppy
in the given model parameter space, but could be transformed back into a different parameter space where
they would become ``nice" again.
%
One could try to find/approximate such a transformation ``back to nonsloppiness" - but then the new parameters
in this transformed space might have no obvious physical meaning or interpretation. 
%
More interestingly, one might try to uncover such a transformation through jointly data-mining parameter
space and model prediction space \em{with the right metric}; in our opinion, the effort to construct a meaningful
metric with which to navigate model-prediction space (and through it, possibly, also model parameter space)
is an important hallmark of the model reduction effort of Transtrum and coworkers.

When one discusses reduction of variables as well as parameters in physical models, the first technique
that arguably comes to mind is dimensional analysis - finding (through the Buckingham $\pi$ theorem)
nondimensional groupings of variables and parameters in terms of which the model can be expressed.
%
In this work we will not attempt to incorporate this last approach in our data-driven framework; we are
interested in detecting effective parameters, and possibly effective variables; we will not explore
methods that involve joint reduction in terms of mixtures of both. 


In presenting the theory, it is initially convenient to consider (a) choosing the number and nature of
measurements to be made and then (b) process \em{all possible} predictions for \em{all possible} 
parameter values together, globally identifying \em{all possible} limiting cases in which the
problem reduces (in possibly different ways). 
%
Several literature papers proceed in this form.
%
We are interested in computational navigating parameter space, taking into account the ``local"
sloppy directions: finding ways to usefully parametrized an appropriately transformed parameter
space so that it can be simply -and hopefully efficiently- searched by local Taylor series
searches (in the right effective directions). 
%
If successful, this would provide a ``parameter free" component of our equation-free/variable-free
framework: both state variables, parameters and the right hand sides of the relevant evolution equations
would be determined in a data-driven manner.
%
We also mention that we believe there is a strong connection (on the data-mining algorithm side) between
what we attempt to do here (the jointly-informed reduction of parameter space and state space) and the
recent developments in the geometry and analysis of dual networks on questionnaires \cite{A14}.


{\color{gray}
\paragraph{Sloppiness.}
Holy grail = modeling $\xrightarrow{data}$ calibration
$\xrightarrow{wrappers}$ understanding/prediction/aggregate
statistics/automated tasks (EF).
Field reality = parameter sloppiness:
compensatory nature of individually meaningful
(at detailed/fine level) parameters.
Parameter sloppiness in the literature:
Sethna, Transtrum, even Constantine.
Distinguish sloppiness from overparameterization.\footnote{`{\it The
origin of sloppiness is not a simple lack of data
where trivial overparametrization leads to unidentifiable parameters.}'
\cite{WCGBMBES06}}
Liken sloppiness to perturbation:
singularly (also regularly?) perturbed systems
are indifferent to changes in (fine) initialization
if macro-quantities are kept invariant.

\paragraph{Problem statement.}
Need non-sloppy effective parameters
to reduce complexity/effectuate tractability/explore system features
in parameter space.
Draw analogy to evolution laws in terms of effective state variables.
Carry analogy further:
detailed models transparent (few simple rules) but large,
coarse-grained models insightful but non-trivial to derive;
individual parameters easily interpretable but sloppy,
effective parameters non-sloppy but non-trivial to derive and interpret.
Re-iterate problem statement.

\paragraph{Problem solution.}
Suggested partial solution: data-driven identification of effective parameters.
[Yannis. Include literature.]

\paragraph{Peripherals.}
Several considerations arise in the process:
some new, some recast in a new light.
(a) Helpful/insightful transformations of parameters.
(b) Duality (of some sort) between state-/parameter space.
(c) Contrasting singular/regular perturbations and their phenotypes.

\paragraph{Extensions.}
[Yannis.]
}



\section{Meaning and origin of sloppiness}

\subsection{Setting}
%
We frame our discussion in the context of multivariable vector functions $x(t;\p) \in \R^d$.
The dimension $d$ is problem-specific and thus arbitrary.
We have partitioned the set of independent variables in two parts, the bona fide variables $t = (t_1,\ldots,t_K) \in I \subset \R^K$ and the parameters $\p = (\p_1,\ldots,\p_M) \in \ps \subset \R^M$, with the dimensions $K$ and $M$ likewise arbitrary.
In this manuscript, we restrict $x$ to be the solution to an initial/boundary value problem for a set of differential equations posed over some domain $I$ and thus known only implicitly.
%In such a setting, the independent variables are split into $t$'s and $\p$'s according to whether derivatives of $x$ with respect to them appear in the differential equations or not.
In our examples below we deal with the case $t\in\R$, in which $x$ satisfies an ODE system and $\p$ will enters the problem parametrically.\\

To investigate the effect of parameter variations on model observations, we write $\fmr : \ps \to \fms$ for the map $\p \mapsto \{ x(t;\p) \}_{t \in I}$ sending specific parameter values to \emph{full model responses} and assume it to be injective.
For each fixed $\p\in\ps$, the full model response $\fmr(\p) = \{x(t;\p) \}_{t \in I}$ is an element of some appropriate function space $\fms$ termed the \emph{full model space}.
As $\p$ ranges over all possible values in $\ps$, the response $\fmr(\p)$ traces out the (generically $M-$dimensional) \emph{full model manifold} $\fmm =
%\mathrm{graph}(\fmr)
%=
\{ \fmr(\p) \}_{\p \in \ps}
\subset
\fms$.
We will assume that manifold to be $C^1$, i.e. $x(t;\p)$ to be continuously differentiable with respect to parameters.\\

Observing the \emph{full} model response amounts to monitoring the \emph{full} system state for \emph{all} $t \in I$.
One is only rarely interested in that, observing instead one or more \emph{functionals} $f^*_1 , \ldots , f^*_N : \fms \to \R$ of that response.
For the purposes of our discussion, each such functional can be thought of as \emph{constraining} $x$ in some way---e.g., as returning the value $x_j(t_i)$
of a state vector component at some specific time.
For any $\p\in\ps$, we term each individual $f^*_n(\fmr(\p))$ a \emph{partial observation} and the $N-$tuple
%
\[
 \mu(\p)
=
 f^*(\fmr(\p))
=
\left[\begin{array}{c}
 f^*_1(\fmr(\p)) \\ \vdots \\ f^*_N(\fmr(\p))
\end{array}\right]
\in
 \R^N
\]
%
an \emph{(observed) model response}.
Under the action of $f^*$, the full model manifold $\fmm$ is projected to
the \emph{(observed) model manifold} $\omm = \{ \omr(\p) \}_{\p \in \ps}$
in the \emph{(observed) model space} $\R^N$.
We will assume $\omr : \ps \to \omm$ to be a homeomorphism,
which makes it an atlas for the model manifold
and excludes the possibility of unidentifiable parameters.
This specifically requires that $N \ge M$, i.e. that we observe at least as many functionals as there are system parameters.\\
%For any fixed observation $z = (z_1,\ldots,z_N) \in \omm$ and $1 \le n \le N$,
%the individual fiber $\fiber_{n,z_n} = {f^*_n}^{-1}(z_n)$
%is of \emph{co-dimension one} in $\fms$
%and thus intersects the full model manifold $\fmm$ (generically) along
%an $(M-1)$-dimensional submanifold $\fmm_{n,z_n} = \fiber_{n,z_n} \cap \fmm$.
%%for linear functionals $f^*_n$,
%%it is an affine copy of $\mathrm{Ker}(f^*_n)$.
%This submanifold corresponds, in turn, to a region
%$\ps_{n,z_n} = \fmr^{-1}(\fmm_{n,z_n})$ of the same dimension
%in parameter space $\ps$.
%\emph{All parameter combinations in that region yield model responses
%consistent with the given partial observation $z_n$
%in that $(f^*_n\circ\fmr)(\ps_{n,z_n}) = z_n$.}
%Increasing the number of fixed partial observations one at a time,
%one decreases the dimension of the parameter space region consistent with them until,
%at $N=M$ and further, that becomes zero.
%This is evident in that $\D_\p \omr$ is of rank $M$
%and $\omr : \ps \to \omm$ is one-to-one.

Discussing sensitivity to parameter variations means assessing the \emph{magnitude} of shifts in model responses relative to such variations.
We measure distances on $\omm$ using a Riemannian metric effectively induced by the norm in model space $\R^N$ \cite{TMS11}.
We equip that space with the standard Euclidean norm $\norm{\cdot}$, sidestepping in the process many important questions relating to weighing and independence of partial observations.
In that setting, an infinitesimal displacement
$d\p = (d\p_1 , \ldots , d\p_M)^{\rm T}$ in $\ps$
yields the infinitesimal displacement
$dz = (dz_1 , \ldots , dz_N)^{\rm T} = (\D_\p\mu) d\p \in \mathrm{T}_\p\omm$,
with length $\norm{dz}^2 = (d\p)^{\rm T} \, g \, d\p$.
Since $\D_\p \omr$ is assumed of full rank, the $M \times M$ matrix $g = (\D_\p\omr)^{\rm T}(\D_\p\omr)$ is positive definite and represents the \emph{metric tensor} for $\omm$ for the specific atlas $\omr$.
Although the system responds to \emph{all} parameter variations $d\p$ local to an arbitrary point $\p \in \ps$, the observed model response may vary greatly with the \emph{direction} of $d\p$.
\emph{Sloppiness} arises when responses become \emph{disparate} along certain directions, making the metric in parameter space a poor predictor of model response.
This situation leaves a commensurate mark on the spectrum of the metric in the form of disparate eigenvalues, the smallest of which correspond to sloppy responses.
The pull-backs under $\D_\p \omr$ of the eigendirections
%in $\mathrm{T}_\p\omm$
associated with these eigenvalues are, in turn, the sloppy directions in $\ps$.
Although standard techniques can identify sloppy directions locally, large deviations from the reference parameters lead to crippling nonlinearities.
To move beyond local sensitivity analysis and find globally effective parameters, one must employ other, nonlinear data mining techniques.



\subsection{Origins of sloppiness}
\label{ss-origins}
%
One of the ways in which sloppiness arises is through an ill-conditioned set of observation functionals.
Indeed, $\omm$ locally has dimension equal to $\mathrm{rank}(\D_\p \omr)$, so \emph{exact} linear dependencies among functionals reduce its dimensionality.
A continuous deformation of a linearly dependent such set yields an ill-conditioned one, fleshing out in the process a model manifold with a hyper-ribbon structure.
Pictorially, this is a process similar to inflating an air mattress.\\

To study systematically the factors contributing to sloppiness, we investigate how
%the projections
$f^*_1,\ldots,f^*_N$ transform the metric $h = (\D_\p\fmr)^{\rm T}(\D_\p\fmr)$ on $\fmm$ to the metric $g = (\D_\p\omr)^{\rm T}(\D_\p\omr)$ on $\omm$.
For clarity of presentation, we restrict our attention to the case where
$f^*_1,\ldots,f^*_N$ are linear and linearly independent, while $\fms$ is a Hilbert space with inner product $\langle\cdot,\cdot\rangle$.
%and induced norm $\norm{\cdot}_\fms$.
In that setting, $\fms$ is isomorphic to its dual $\fms^*$, so there exist $f_1,\ldots,f_N \in \fms$ satisfying $f^*_n = \langle f_n,\cdot \rangle_\fms$.
It follows that
%
\[
 \omr(\p)
=
\left[\begin{array}{c}
 \langle f_1,\fmr(\p) \rangle
\\
\vdots
\\
 \langle f_N,\fmr(\p) \rangle
\end{array}\right] ,
\ \mbox{with push-forward} \
 \D_\p\omr%\vert_\p
=
\left[\begin{array}{c}
 \langle f_1,\cdot\rangle
\\
\vdots
\\
 \langle f_N,\cdot \rangle
\end{array}\right]
 \D_\p\fmr,%\vert_\p ,
\]
%
so the metric on $\omm$ becomes
%
\be
 g
=
 (\D_\p\fmr)^{\rm T} f f^* (\D_\p\fmr) . % skipping \vert_\p
\label{g-vs-Dpx}
\ee
%
This relates $g$ to $h$ through the mediation of the operator $f f^* : \fms \to \fms^*$, which is shorthand for
%
\[
\begin{array}{c}
 f f^*
=
\\
\vspace*{4mm}
\end{array}
\begin{array}{c}
\Big[
 \langle f_1,\cdot\rangle
,
 \ldots
,
 \langle f_N,\cdot\rangle
\Big]
\\
\vspace*{4mm}
\end{array}
%
\left[\begin{array}{c}
 \langle f_1,\cdot\rangle
\\
 \vdots
\\
 \langle f_N,\cdot\rangle
\end{array}\right] ,
\quad\mbox{i.e.}\ \
 \fms \ni \mathrm{x} \mapsto \sum_{n=1}^N
 \langle f_n , \mathrm{x}\rangle
 \langle f_n , \cdot \rangle \in \fms^* .
\]
%

The effect of that mediation is best understood by using the isomorphism between $\fms$ and $\fms^*$ to interpret $f f^*$ as a symmetric endomorphism on $\fms$.
The spectrum $\sigma(f f^*)$ then consists of the zero eigenvalue, linked to the co-dimension $N$ kernel $\mathrm{Ker}(f f^*) = f_1^\perp \cap \ldots \cap f_N^\perp$, and of a nontrivial part linked to the invariant subspace $\mathrm{Im}(f f^*) = \mathrm{span}(f_1,\ldots,f_N)$.
Restricted on that subspace and expressed with respect to $f_1,\ldots,f_N$, the map $f f^*$ is represented by the $N \times N$ Gramian
%
\[
 f f^*\vert_{\mathrm{Im}(f f^*)}
=
 f^* f
=
\left[\begin{array}{ccc}
 \langle f_1,f_1\rangle
&
 \ldots
&
 \langle f_1,f_N\rangle
\\
 \vdots
&
 \vdots
&
 \vdots
\\
 \langle f_N,f_1\rangle
&
 \ldots
&
 \langle f_N,f_N\rangle
\end{array}\right] .
\]
%
To exploit this decomposition of $f f^*$ in \eqref{g-vs-Dpx}, we also decompose $\D_\p\fmr$ into components along $\mathrm{Ker}(f f^*)$ and $\mathrm{Im}(f f^*)$,
%$\D_{\p_1}\fmr\vert_\p , \ldots , \D_{\p_N}\fmr\vert_\p \in \mathrm{T}_{\fmr(\p)}\fmm \simeq \fmm$
%
\be
 \D_\p\fmr(\p)
=
 \mathrm{N}(\p)
+
 \left[ f_1 , \ldots , f_N \right]
 \mathrm{I}(\p) .
\label{Dpx-decomp}
\ee
%
This decomposition is meant component-wise, i.e. $\D_{\p_m}\fmr = \mathrm{N}_m +  \sum_{n=1}^N \mathrm{I}_{nm} f_n$ with $\mathrm{N}_m\in\mathrm{Ker}(f f^*)$.
Substitution of \eqref{Dpx-decomp} into \eqref{g-vs-Dpx} now yields the final result,
%
\be
 g%\vert_{\omr(\p)}
=
 \mathrm{I}^{\rm T} % skipping (\p)
\left(
 f^* f
\right)^2
 \mathrm{I}
=
 \big[\left(f^* f\right) \mathrm{I}\big]^{\rm T}
 \big[\left(f^* f\right) \mathrm{I}\big] , % skipping (\p)
\label{g-ito-proj}
\ee
%
expressing the metric on $\omm$ as the Gramian of a product.
The first term in this product is the Gramian of the functionals, whereas the second expresses the projection of $\mathrm{T}_{\fmr(\p)}\fmm$ on the space these span.\\

Equation~\eqref{g-ito-proj} makes it evident that both $f^* f$ and $I$ contribute to $\sigma(g)$ and thus potentially to sloppiness.
The first matrix depends on the pairwise norms and angles of $f_1,\ldots,f_N$, and its spectrum characterizes their relative weight and degree of independence.
Sloppiness, in this case, is due to functionals that are poorly scaled (disparate norms) or nearly linearly dependent (disparate angles).
This was the case in \cite{WCGBMBES06}, where sloppiness was mediated by a Vandermonde matrix specific to Taylor polynomials.
%Replacing those with any orthonormal polynomial basis would have presumably sufficed to remove it.
The second route to sloppiness relates to the $N \times M$ matrix $\mathrm{I}$.
For sloppiness to arise through that matrix, there must be directions in $\mathrm{T}_\p\fmm$ that are nearly parallel to the projection kernel $\mathrm{Ker}(f^*) = f_1^\perp \cap \ldots \cap f_N^\perp$ of the functionals.
Here, the functionals may be well-scaled and the \emph{full} model response to parameter variations appreciable, but certain directions in parameter space evoke `unproductive' full model responses.
These practically lie in $\mathrm{Ker}(f^*)$ and are thus projected to much smaller \emph{observed} model responses, making the setup sloppy.
Prime such examples are multiscale systems, in which many parameter directions only affect behavior at unobserved scales.\\

{\color{gray}\paragraph{Caricature model space.}
Distinguish output from cost function: former fixes model (=data)
space, latter space norm (i.e., packages data into a scalar).
\emph{Output} analogous to random variable: constrains detailed system
output (i.e., detailed state at all times).  In effect, output formed
by projecting detailed system output (=a point in some space) to ($N$
in number) $1-$D subspaces $\rightarrow$ observables are projections
(along co-dim. $N$ kernel intersection) of system output onto an
$N-$dim subspace [nonlinear kernel fibrations OK].  Model manifold
constructed as projection of `detailed model manifold.'  Norm in model
space $\rightarrow$ semi-norm in `detailed model space': balls
$\rightarrow$ cylinders (ball $\times$ kernel) $\rightarrow$
``neutral" directions along projection kernel.  Sloppy directions in
parameter space are mapped to neutral (i.e.kernel) directions on
`detailed model manifold', i.e. $d\theta$ is (largely) along
intersection of kernels $\rightarrow$ maps to much thinner
displacement on (restricted) model space.  Conclusion: lower-dim
subsets of parameter space may map to directions along kernel, but
these do not necessarily extend through entire parameter space:}
discuss two cases: (a) sing. pert. caricature with $\eps\in\R$ (only
$1/\eps$ above threshold maps to kernel directions, whereas $1/\eps$
below it---incl. negative values---does not: detailed model manifold
is 'sigmoidal') and (b) Goldbeter--Koshland modules, where steady
states (observables) show `sloppiness--sharp transition--sloppiness'
structure.  {\color{gray}Eye of the beholder again: metric in parameter space bad
predictor of (constrained) model response.}



\section{Introductory examples}
%
In the last section, sloppiness was introduced and analyzed as a \emph{local} property pertaining to the specific combination of model and observables.
Sloppy behavior, however, is an \emph{open property} characterizing entire regions of parameter space.
This has strong implications for sloppy setups, foremost among which that the observables are affected by a \emph{reduced} number of effective system parameters.\\

To understand this, we remark that the footprint of sloppiness in parameter space is the existence of \emph{neutral sets}, all points in which evoke \emph{nearly identical} observed model responses.
Such regions are extended, effectively lower-dimensional and arranged so as to roughly foliate parameter space; effective system parameters on them remain approximately constant.
In the linear picture we painted above, an $M-$dimensional ball around $\omr(\p)$ is pulled back to an ellipsoid centered at $\p$ with widely disparate principal axes.
The smallest principal directions correspond to sloppy parameter combinations, and they can be unraveled by Linear Algebra techniques such as Principal Component Analysis \cite{ADS06}.
In reality, pronounced deviations of $\omr$ from linearity mean that neighborhoods of $\omr(\p)$ are mapped to \emph{curved} neutral sets in parameter space with disparate characteristic length scales.
These are not directly amenable to linear techniques, requiring instead the development of a nonlinear data mining framework to identify them and the associated effective parameters.
We will make these ideas concrete below through a series of examples.\\

\subsection{An elementary example}
\label{ss-elem.ex}
%
We begin with the prototypical nonlinear dynamical system
%
\be
\begin{array}{rcl}
 \dot{x} &=& y - \lambda x ,
\vspace*{1mm}\\
 \eps \dot{y} &=& \eps x - \displaystyle\left(1+\frac{10}{1.5-\sin y}\right) y ,
\end{array}
\ \mbox{supplemented with} \
\begin{array}{rcl}
 x(0) &=& x_0 ,
\vspace*{1mm}\\
 y(0) &=& y_0 .
\end{array}
\label{elem-ODE}
\ee
%
Dots denote differentiation with respect to time $t \in I = [0,\infty) \subset \R$, so $K=1$.
This model has a unique, globally attracting steady state at the origin
whose stability specifics are controlled by $\eps$ and $\lambda$.
All four of $(\eps,\lambda,x_0,y_0)$ can be viewed as parameters,
but we reduce their number to $M=2$ by fixing $\lambda= 2$ and $x_0 = 1$ and setting $\p = (\eps,y_0)$.
The full model response is the full system trajectory, $\fmr(\p) = \{(x(t;\p),y(t;\p))\}_{t \in I}$, so $d = 2$ and $\fms = C^1(0,\infty) \times C^1(0,\infty)$ is infinite-dimensional.
We finally choose the observed model response
%
\be
 \mu(\p) = \big( y(t_1;\p) \,,\, y(t_2;\p) \,,\, y(t_3;\p) \big) ,
\ \mbox{for fixed instants} \
 0 < t_1 < t_2 < t_3 .
\label{elem-mu}
\ee
%
This setup yields a $2-$D model manifold embedded in $3-$D Euclidean space, see Fig.~\ref{f.elem.ex.1} for a segment of this highly nonlinear manifold.\\

As emphasized earlier, different regions of $\omm$ may or may not be sloppy.
For the setup described in \eqref{elem-ODE}--\eqref{elem-mu}, sloppiness arises in the regime $\eps \ll 1$ where the system is \emph{singularly perturbed}.
In that regime, and during an initial fast transient, all trajectories enter an $\Ord(\eps)$ neighborhood of the $x-$axis containing an attracting, global slow manifold.
They subsequently remain exponentially close to that manifold, so that their fast component $y$ remains uniformly $\Ord(\eps)$.
In that manner, information on each trajectory's \emph{initial} fast component is effectively \emph{erased} during the transient, making $y_0$ a sloppy parameter.
This last observation is crucial in understanding the emergence of sloppiness in multiscale settings.
As $\eps \downarrow 0$, the slow phase sets on earlier and $t_3 > t_2 > t_1$ fall in it one after another.
When this happens for e.g. $t_3$, the partial observation $y(t_3)$ collapses to zero irrespectively of the $y_0-$value that generated it.
This process is evident in the right panel of Fig.~\ref{f.elem.ex.1}.
As $\eps\downarrow0$, the outputs generated by \emph{all} initial fast components within a bounded interval collapse to the origin, as  $y(t_3)$, $y(t_2)$ and $y(t_1)$ become zero one after another.
This situation generalizes to generic multiscale systems with an attracting slow manifold and a fast fibration over it.
Plainly, points in compact subsets of a fast fiber possess matching outer solutions and thus generate similar observations in the slow timescale.\\

Figure~\ref{f.elem.ex.2} shows the neutral set in parameter space for a particular observation, advertised above as the hallmark of sloppiness.
Specifically, we have plotted a portion of the model manifold $\omm$ together with a point $p^* = \omr(\p^*)$ in its sloppy regime and a $\delta-$neighborhood (ball) $\mathrm{B}_\delta(p^*)$ around it.
The ball encloses the submanifold $\omm \cap \mathrm{B}_\delta(p^*)$ whose inverse image under $\omr^{-1}$, shown in the right panel, constitutes the \emph{neutral set $\ps_\delta(\p^*)$ at level $\delta$}; all parameter values in it yield observed model responses $\delta-$close to $p^*$.
This set is specific to our choice of model space metric and grows with $\delta$, namely from $\ps_0(\p^*) = \{\p^*\}$ to a ``pinched'' elliptic disk.
This chirality reflects that smaller $\eps-$values can accommodate wider intervals of $y_0$ within the same tolerance $\delta$, as they yield stronger fast contraction.
For $\delta$ past a certain threshold, $\ps_\delta(\p^*)$ opens up and becomes unbounded.
Evidently, that threshold is reached when the ball $\mathrm{B}_\delta(p^*)$ reaches the $\eps=0$ boundary of $\omm$.\\

A corollary of this last observation is that, as $p^*$ moves towards the $\eps = 0$ boundary of $\omm$, the critical tolerance $\delta$ at which $\ps_\delta(\p^*)$ becomes unbounded vanishes.
This is evident in Fig.~3 (do we want such a figure?), where we have plotted the neutral sets $\ps_\delta(\p^*)$ corresponding to various points $\p^*$ and tolerances $\delta$.
A direct implication is that the system is effectively \emph{parameter-free} in the singularly perturbed regime, as all parameter combinations in a wide set yield responses within tolerance.\\

THINGS TO DO:\\
(1) Color left panel of first figure by $1/\eps$ (not $\eps$); extend range of $y_0$; add color bar.\\
(2) Chop wing tail in second figure.\\
(3) Add panel to second figure with contours. For that, we'll need Alexander's pseudo-arc length continuation algorithm.\\
(4) Add two-panel third figure with contours in parameter space for two $p^*$ (differing only in $\eps$, not in $y_0$) and various $\delta$. Use the same coloring for the level sets (and add color bar), so that it's clear at which $\delta$ level sets open up.

%Interestingly, the dimensionality of the neutral set in this system
%changes depending on the parameter $\epsilon$.
%This is represented by Fig. \ref{fig:sing-pert:x20}.
%In this regime, the neutral set is two-dimensional: neither $\eps$ nor
%$x^2_0$ affect the observed model response, i.e. $x^2$ at $t_1$, $t_2$ and $t_3$.
%However, as $\eps$ increases, we approach a state in which predictions are nearly
%constant for different $x^2_0$, but further changes in $\eps$ does in fact affect $\mu$ as we begin sampling off the slow manifold.
%In this case, we have a one-dimensional neutral set: $\eps$.
%Finally, when $\eps \approx O(1)$, both parameters significantly change $\mu$ and our
%neutral set is empty.
%Fig.~\ref{fig:sing-pert-slopp} captures each of these three regimes.

%
\begin{figure}[t]
\scalebox{0.24}[0.24]{
\includegraphics{clrd_by_params_no2.pdf}
}
\caption{\label{f.elem.ex.1}
The observed model manifold $\omm$ for system~\eqref{elem-ODE} in $3-$D \emph{(observed) model space}.
The two parameters here are $\eps$ and $y_0$, and the map $\omr$ from  parameter to model space is given in~\eqref{elem-mu}.
The manifold is colored by each parameter to visualize how these vary on it.
The manifold bottom corresponds to the singularly perturbed regime $0 < \eps \ll 1$, see left panel.
In that regime, widely different initial conditions yield nearly identical model responses, see right panel.}
\end{figure}
%

%
\begin{figure}[t]
\scalebox{0.24}[0.12]{
\includegraphics{clrd_by_eps.pdf}
}
\caption{\label{f.elem.ex.2}
Same manifold as in Fig.~\eqref{f.elem.ex.1} seen from a different angle.
The green submanifold contains all points within distance $\delta$ from a point $\p^* \in \omm$ (black dot), and it is the intersection of $\M$ with the $\delta-$ball centered at $\p^*$ (light blue ball).}
\end{figure}
%

{\color{gray}\paragraph{Caricature $1 \rightarrow 0$.}
Setting: planar singularly perturbed system + data in sloppy $\eps-$regime.
Limit $\eps \downarrow 0$ entirely removes parameter without sacrificing output accuracy.
Outcome: parameter-free system.}

{\color{red}\paragraph{Caricature cost function.}
Discuss how cost function determines neutral set.
Include short discussion on a priori knowledge of constants
possibly lifting sloppiness (cite fwd to MM).
Discuss free vs constrained (e.g. log-likelihood) choice of cost function.
Cost function choice should be motivated
partly by interest (e.g. which components to include and how)
and partly by statistics/data fitting concerns
(e.g. log-likelihood $\rightarrow$ quadratic sum).}



\subsection{Sloppiness and the eye of the beholder}
\label{ss-beholder}
%
Contrary to parameter identifiability which is a binary notion (a parameter is either identifiable or not), sloppiness is \emph{relative} and affected by various factors, the choice of model parameters and associated parameter scales featuring prominently among them.
This is already evident in the model of Section~\ref{ss-elem.ex},
where we found that $1/\eps$ was sloppy.
There, for fixed initial conditions and any preset threshold $\delta > 0$, there exists $\eps_\delta \ll 1$ such that all model responses generated by any $\eps \in (0,\eps_\delta)$ are $\delta-$close to each other.
In terms of $1/\eps$, the neutral set is the unbounded interval $\ps_\delta = (1/\eps_\delta,\infty)$.
In terms of $\eps$, however, that interval is radically shrunk to $[0,\eps_\delta)$.
This demonstrates that sloppiness can be intrinsically nonlinear and is not solely the result of poor scaling choices; here, the unboundedness of $\ps_\delta$ can only be removed by a bona fide nonlinear transformation such as $\eps \mapsto 1/\eps$ above.\\

We stress this point further by means of a contrived but elucidating example.
Starting from the linear, singularly perturbed, analytically tractable ODEs
%
\be
\begin{array}{rcl}
 \dot{X} &=& -\lambda X ,
\\
 \eps \dot{Y} &=& - Y ,
\end{array}
\quad\mbox{with}\ \eps \ll 1 ,
\label{XY-system}
\ee
%
we construct a nonlinear, coupled ODE system for $(x,y)$ through the transformation
%
\[
 X = x - \beta y^2
\quad\mbox{and}\quad
 Y = y - \alpha x^2 .
%\label{xy-vs-XY}
\]
%
Here, $\alpha$ and $\beta$ are additional parameters.
This re-coordinatization of the phase plane maps the global slow manifold to $y=\alpha x^2$ and the family of fast fibers to $\{x=\beta y^2 +c \ \vert \ c\in \R\}$; the corresponding timescales are $1/\lambda$ and $1/\eps$.
In all simulations below, we fix $\eps = 10^{-3}$ and $\beta = 10^{-2}$ with initial conditions $x(0) = y(0) = 1$.
As model response, we choose
%
\begin{align}
 \omr(\alpha, \lambda)
=
\big(
 x(t_1) , \ldots , x(t_{10}) , y(t_1) , \ldots , y(t_{10})
\big)
\in
 \mathbb{R}^{20} ,
\label{eq:henon-mr}
\end{align}
%
where times are evenly spaced on the time interval $[0.1, 1.0]$.
In this context, the remaining parameters $\alpha$ and $\lambda$ are not sloppy.
When poorly transformed, however, they can exhibit highly nonlinear sloppiness.\\

To starkly demonstrate this, we define new parameters $(\p_1,\p_2) =
(H \circ H)(\alpha,\lambda)$, with $H : \R^2 \to \R^2$ the H\'enon map
tuned to its chaotic regime, $(\alpha, \lambda) \mapsto (1 - 1.3 \alpha^2 + \lambda , 0.3 \alpha)$.
\textcolor{red}{[why 1.3 instead of the classical 1.4?]}
With a slight abuse of notation, we will be writing $\omr(\alpha, \lambda)$ instead of $\omr(\p(\alpha, \lambda))$.
%In terms of these transformed variables, our model becomes
%%
%\begin{align}
%  \begin{bmatrix} \dot{x} \\ \dot{y} \end{bmatrix}  = \frac{1}{1 -
%  4 \bigg( \frac{\p_1}{b} - 1 + a \bigg( \frac{k}{b} \bigg)^2 \bigg) \beta x y} \begin{bmatrix} 1 &
%  2\beta y \\ 2 \bigg( \frac{\p_1}{b} - 1 + a \bigg( \frac{k}{b} \bigg)^2 \bigg) x  &
%  1 \end{bmatrix} \begin{bmatrix} - \frac{k}{b}(x - \beta y^2) \\ -(y -
%  \p_1)/\epsilon \end{bmatrix} 
%\label{m.henon}
%\end{align}
%%
%with $k = {\p_2 - 1 + a
%  \big(\frac{\p_1}{b}\big)^2}$.
Following the procedure outlined in the last section, we then fix $(\alpha^*,\lambda^*) = (1,1)$ and $\delta = 1.4$, compute the transformed parameter pair $\p^* = (H \circ H)(\alpha^*,\lambda^*)$ and corresponding base response $\omr^* = \omr(\p^*)$ and, finally, map out the neutral set at level $\delta$ by mining $\p-$values with responses $\delta-$close to $\omr^*$.
Mining, here, proceeds through an iterative least-squares optimization routine with objective function $\left\| \omr(\p) - \omr^* \right\|^2$ and tolerance $\delta^2$.
Starting from some parameter setting $\p^{(0)}$, the routine generates a sequence $\{\p^{(1)},\p^{(2)},\ldots\}$ until that objective function drops below tolerance for some $\p^{(n)}$.
Different initializations $\p^{(0)}$ yield different points $\p^{(n)}$ in the neutral set, so that one can map out that set by varying the initialization.\\

The result of this process is shown in Fig.~\ref{f.transf-params}.
%Figure~\ref{f.henon}
The left panel shows the output of our model fitting in $\p-$plane, whereas
%Fig.~\ref{f.henon-inverse}
the right one reveals what this collection looks like in terms of the original parameters $(\alpha,\lambda)$.
While the latter is not sloppy as expected, the former plainly is: the characteristic length scales of the domain differ by no less than two orders of magnitude.
This demonstrates that, in certain cases, an appropriate \emph{nonlinear} transformation of the model parameters can remove sloppiness.

%
\begin{figure}[ht!]
 \begin{subfigure}[t]{0.49\textwidth}
  \centering
  \includegraphics[width=\textwidth]{transformed-params-fromoptimization-insert}
% \subcaption{Parameter combinations obtained by least-squares optimization with various initializations. \label{f.henon}}
 \end{subfigure}
 \begin{subfigure}[t]{0.49\textwidth}
  \centering
%    \includegraphics[width=\textwidth]{inverted-params}
  \includegraphics[width=\textwidth]{transformed-params-fromoptimization-insert}
% \subcaption{Original parameter values obtained by inverting the data set in the left panel. \label{f.henon-inverse}}
 \end{subfigure} %
\caption{
The set $\ps_\delta(\p^*)$ in terms of the transformed parameter set $\p$ (left panel) and of the original one $(\alpha,\lambda)$ (right panel).
Here, $\delta = 1.4$, $\p^*$ corresponds to the point $(\alpha_*,\lambda_*) = (1,1)$ and the model response is given in Eq.~\eqref{eq:henon-mr}.
All points were obtained through least-squares optimization starting from random initial conditions.
The absence of sloppiness is evident in the former, whereas the domain appears bent and sloppy in the latter.
\label{f.transf-params}}
\end{figure}

{\color{gray}\paragraph{Overall plan.}
Same setting.  Map $\eps \mapsto 1/\eps$ maps interval
$[0,\eps_{\rm max}]$ to ray $\rightarrow$ sloppiness not removable
through linear rescaling.  [Here again, transformations.] Carry on:
flat neutral set $\mapsto$ spiral/Swiss roll.  Outcome: sloppiness can
be more than a finite stretching even in the absence of singular
perturbations.}\\



\section{Data mining of sloppy systems}
\label{s-data.mining}
%
In the example treated in Section~\ref{ss-elem.ex}, we saw how a sloppy setup can reduce the effective dimensionality of parameter space.
Depending on the regime, the degree of reduction varied from none (no sloppiness) to intermediate (one neutral direction) to full (parameter-free system).
In that example, also, sloppy directions aligned well with problem parameters, so that first $y_0$ and then both $y_0$ and $\eps$ became sloppy as one approached the manifold boundary.
In general, though, sloppy directions do not align well with model parameters and neutral sets can be highly nonlinear, cf. Section~\ref{ss-beholder}.
This creates a specific need for algorithms that identify neutral sets and complementary effective parameters in parameter space.
In this section, we demonstrate such an algorithm by treating a system with a three-dimensional parameter space, in which neutral sets are effectively two-dimensional.
Our treatment is self-contained but acts as precursor to our more involved discussion of the Michaelis--Menten system in Section~\ref{s-enzyme}.

{\color{gray}\paragraph{Motivation.}
(re-)introduce neutral set in parameter space.  Motivate data mining
of neutral set for effective parameter identification.  Data mining
via PCA done \cite{ADS06} but neutral sets extended so nonlinearities
pronounced $\rightarrow$ nonlinear data mining such as diffusion
maps.}



\subsection{A simple chemical kinetics model}
\label{ss-321}
%
\paragraph{Overall plan.}
$3 \rightarrow 1$ parameters. DMAPS
$\rightarrow$ sloppy set $\rightarrow$ effective parameter. Outcome:
nonlinear data mining + transversal propagation $\rightarrow$
effective parameters.  Also discuss non-uniqueness of effective
parameters (anything transversal); candidate parameter combinations
are tested through non-degeneracy of Jacobian.
Alexander's Model with plain vanilla and mixed kernel.\\

We consider the first-order chemical reaction system
%
\begin{align*}
  X \xrightleftharpoons[k_{-1}]{k_1} Y \xrightarrow[]{k_2} Z
\end{align*}
%
where all steps are assumed elementary.
The time evolution of the concentrations of the constituents is subject to mass action kinetics, resulting in the linear ODE system
%
\be
\dot{\left[\begin{array}{c}
 x \\ y \\ z
\end{array}\right]}
=
\left[\begin{array}{ccc}
 -k_1 & k_{-1} & 0 \\
 k_1 & -(k_{-1}+k_2) & 0 \\
 0 & k_2 & 0
\end{array}\right]
%
\left[\begin{array}{c}
 x \\ y \\ z
\end{array}\right]
%
\ \mbox{with} \
%
\left[\begin{array}{c}
 x(0) \\ y(0) \\ z(0)
\end{array}\right]
=
\left[\begin{array}{c}
 x_0 \\ y_0 \\ z_0
\end{array}\right] .
\label{xyz-ODE}
\ee
%
Since $\dot{x}+\dot{y}+\dot{z}=0$, the quantity $w_T = x+y+z$ is conserved and the system can be exactly reduced to a planar form.
We fix the parameter set to $\p = (k_{-1},k_1,k_2)$ and monitor product levels at fixed time instants $0 < t_1 < \ldots < t_N$, so that the model response is
%
\[
 \omr(\p)
=
 \big(z(t_1;\p) , \ldots , z(t_N;\p)\big) .
\]
%
Here as well, nearness is measured in terms of the Euclidean norm in $\R^N$; we
comment on other choices (e.g. a maximum likelihood approach) below.\\

To understand sloppiness in this setting, we observe that the solution to \eqref{xyz-ODE} evolves in two disparate timescales, when $\p$ lies in the region $\Omega$ where $k_1 \ll k_{-1}+k_2$.
%$k_2 \gg k_{-1}$
In that regime and past an initial fast transient, the species $Y$ is approximately in quasi-equilibrium.
To leading order, then, the product concentration is given by
%
\begin{align}
 z(t)
=
 w_T \left(1 - \ex^{-k_{\rm eff} t}\right) ,
\ \mathrm{with} \
 k_{\rm eff} = \frac{k_1 k_2}{k_{-1} + k_2} .
\label{keff}
\end{align}
%
If measurement times also lie outside the fast transient, then the resulting dynamics are affected by the \emph{single} effective parameter $k_{\rm eff}$.
Plainly, all three $\p-$components affect the model response, cf. the formula for $k_{\rm eff}$.
Nevertheless, $\Omega$ is foliated by a one-dimensional family of two-dimensional foils $k_{\rm eff} = const.$, with all points on each foil yielding a nearly identical model response.
This approximate but very tangible indeterminacy results in highly nonlinear neutral sets, cf. Fig.~\ref{fig:qssa:sloppy-manifold}.
More explicitly, any combination of the bare parameters $k_1$, $k_{-1}$ and
$k_2$ that yield the same value for $k_{eff}$ will predict the same
system dynamics.
.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{keff-1}
  \caption{The surface $k_{\rm eff} = 1$ in parameter space, cf.~\eqref{keff}. The surface does not align with any coordinate axis, so that system response is strongly affected by changes in single parameters. Coordinated changes in multiple parameters  that keep one on the surface, however, compensate for each other and leave the model response unaltered.
  \label{fig:qssa:sloppy-manifold}}
\end{figure}


It is important to note that this surface in parameter space captures
all the sloppiness hidden in our model. Traversing along the manifold we
find many different parameter combinations with nearly equivalent
model predictions; moving in a direction transverse to the surface
brings us to a new $k_{eff}$ and thus significantly different model
predictions. Unfortunately, real-life systems will typically contain
tens to hundreds of parameters, preventing us from simply plotting the
neutral set and visually inspecting its sloppiness. Instead, this
sloppy surface will be hidden somewhere within the high dimensional
parameter space. Our objective then becomes to uncover the neutral set
in a potentially high-dimensional parameter space and to develop a
parameterization of its potentially nonlinear structure. \\

% In fact, we must first confront a separate challenge: given some
% model, how do we generate its underlying sloppy surface? One could
% simply sample parameter space and keep those points for which the
% objective function fell within some tolerance, but this becomes
% expensive and inefficient in high dimensions. Instead, we achieve this
% aim by repeatedly fitting our model starting from different initial
% guesses in parameter space. When dealing with an ideal model, each
% initial guess would converge to the same, global optimum in parameter
% space. However, sloppiness creates a manifold of solutions, each of
% which yield equally good fits. Thus, by starting our optimization
% routine at different points in parameter space, we will naturally
% converge to different values on the sloppy manifold, eventually
% generating a dense cover as shown in Fig. \ref{fig:qssa:opt}.

First, we must confront the challenge of generating a collection of
sloppy parameters. At present, we fix our model response $\omr^*$ and
implicitly the corresponding parameters $\theta^*$, and then
investigate the set of parameters that map to a model response within
distance $\delta$ of $\omr^*$ by sampling around $\theta^*$. That is,
by sampling parameter space we find points in the set
$S = \{\theta \ssep  \| \omr(\theta) - \omr^* \| < \delta \}$. \\

Given this dataset, we turn to the problem of uncovering its hidden structure. This
problem is naturally framed as one of dimensionality reduction: how
can we more efficiently describe our high-dimensional dataset? One
might initially consider performing a principal component analysis;
however, as Fig. \ref{fig:qssa:sloppy-manifold} shows, our manifold is
highly nonlinear. Instead, we turn to Diffusion Maps (DMAPS), a data mining
technique capable of uncovering such nonlinear structure. \\

\subsubsection{Diffusion Maps}

The general goal of many dimensionality reduction techniques is to generate
an embedding $\Phi : \mathbb{R}^n \rightarrow \mathbb{R}^p$ that maps
each point in the high-dimensional dataset $\{ x_i \}_{i=1}^N \in
\mathbb{R}^n$ to a lower dimensional one $y_i = \Phi(x_i) \in
\mathbb{R}^p$ that, in some way, captures the essential features of
the original dataset in a more succint description. Ideally $p \ll
n$. \\

If the high-dimensional points $\{ x_i \}$ lie on some hyperplane in
$\mathbb{R}^n$, then the well known technique principal component
analysis (PCA) will uncover a basis for this hyperplane, essentially
revealing any linear relationships present in the data. However, if
the data lies not on a hyperplane but rather some nonlinear manifold,
plain PCA will perform poorly. 

DMAPS generates parsimonious descriptions of nonlinear manifolds by
finding approximate eigenfunctions and eigenvalues of a diffusion
process over the dataset, in particular, the approximate
eigenfunctions and eigenvectors of the Laplace-Beltrami operator on
the manifold with homogeneous Neumann boundary conditions. Just as the
exact eigenfunctions parameterize the manifold on which they're
defined, the approximate eigenfunctions (actually eigenvectors) of the
diffusion process on the dataset can likewise be used as a reduced set
of coordinates for the lower-dimensional manifold on which the data
lie. The algorithm is as follows: first construct the symmetric matrix
$W$ with entries

\begin{align*}
  W_{ij} = \exp(-\frac{\| x_i - x_j \|^2}{\epsilon^2}
\end{align*}

Then divide each row by that row's sum to make the row-stochastic
matrix $A$ for which

\begin{align*}
  A_{ij} = \frac{W_{ij}}{d_j}
\end{align*}

where $d_j = \sum_j W_{ij}$. By calculating the eigendecomposition of
$A$ we find eigenvalues
$\lambda_1 = 1 > \lambda_2 \ge \lambda_3 \ge \hdots \ge \lambda_n \ge
0$ and corresponding eigenvectors $\phi_1, \phi_2, \hdots,
\phi_n$. The $k$-dimensional diffusion map of point $x_i$ is then
given by

\begin{align*}
  \Phi_t (x_i) = \begin{bmatrix} \lambda_2^t \phi_2(i) \\ \lambda_3^t
    \phi_3(i) \\ \vdots \\ \lambda_{k+2}^t \phi_{k+2}(i) \end{bmatrix}
\end{align*}

where $\phi_j(i)$ is the $i^{th}$ entry of eigenvector $j$ and $t$ is
a parameter that allows multiscale analysis of the dataset. See
\cite{dmaps} \cite{nadler} for more details.

% description of DMAPS here? %

When we apply DMAPS to the set of parameter combinations found through
repeated optimization, we indeed uncover a parameterization of the
nonlinear system sloppiness. In this case, DMAPS provides a
two-dimensional embedding of the surface, as shown in
Fig. \ref{fig:qssa:dmaps}. \\

However, it would also be useful if we could uncover not only a
parameterization of the sloppy directions in our model, but also the
important parameter directions.

% alternative dmaps description %

\section{Parameters and variables}

\paragraph{Variables as parameters.}
Initial conditions subject to inference $\rightarrow$ link between
parameter space and state space.  Entire fast fiber sloppy.  Fast/slow
are relative notions, fixed by observer's (measurement) timescale,
i.e. by time grid (discrete case) or by initial relaxation window
(continuous case).

\paragraph{Parameters as variables.}
Optimization dynamics in data fitting $\rightarrow$ dynamics in
parameter space. State space dynamics + choice of cost function shape
the landscape in parameter space that the optimization algorithm
navigates.  Relatedly: state space dynamics $\rightarrow$ model
manifold; cost function $\rightarrow$ model space norm.

\paragraph{Parameters as variables.}
Joint parameter-/variable-space.
[What can we contribute here?]


\section{Sloppiness in enzyme kinetics}
\label{s-enzyme}



\section{Metrics}


\section{Discussion}

Discuss model space and state space.\\

Discuss extension by following manifolds.\\

Discuss bounded/unbounded spaces.\\


\begin{thebibliography}{99}
%

\bibitem{AS72}
M.~Abramowitz and I.~A.~Stegun,
Handbook of Mathematical Functions
with Formulas, Graphs, and Mathematical Tables (10th ed.),
National Bureau of Standards, Washington, DC, 1972.

\bibitem{ADS06}
P.~Achard and E.~De~Schutter,
Complex parameter landscape for a complex neuron model,
{\it PLoS Comp. Biol.} \textbf{2(7)} 0794--0804, 2006.

\bibitem{A14}
J.~I.~Ankenman,
Geometry and Analysis of Dual Networks on Questionnaires,
Ph.D. Thesis, Yale University, 2014.

\bibitem{JG11}
K.~A.~Johnson and R.~S.~Goody,
The original Michaelis constant:
translation of the 1913 Michaelis--Menten paper,
{\it Biochem.} \textbf{50} 8264--8269, 2011.

\bibitem{LB34}
H.~Lineweaver and D.~Burk,
The Determination of Enzyme Dissociation Constants,
{\it J. Am. Chem. Soc.} \textbf{56} 658--666, 1934.

\bibitem{MM13}
L.~Michaelis and M.~L.~Menten,
Die Kinetik der Invertinwirkung,
{\it Biochem. Z.} \textbf{49} 333--369, 1913.

\bibitem{SS89}
L.~A.~Segel and M.~Slemrod,
The Quasi-Steady-State Assumption:
A Case Study in Perturbation,
{\it SIAM Review} \textbf{31(3)} 446--477, 1989.

\bibitem{TMS10}
M.~K.~Transtrum, B.~B.~Machta and J.~P.~Sethna,
Why are nonlinear fits to data so challenging?,
{\it ?Phys. Rev. Lett.} \textbf{104} 060201, 2010.

\bibitem{TMS11}
M.~K.~Transtrum, B.~B.~Machta and J.~P.~Sethna,
Geometry of nonlinear least squares
with applications to sloppy models and optimization,
{\it Phys. Rev. E} \textbf{83} 036701, 2011.

\bibitem{TQ14}
M.~K.~Transtrum and P.~Qiu,
Model reduction by manifold boundaries,
{\it ?Phys. Rev. Lett.} \textbf{113} 098701, 2014.

\bibitem{WCGBMBES06}
J.~J.~Waterfall, F.~P.~Casey, R.~N.~Gutenkunst, K.~S.~Brown, C.~R.~Myers,
P.~W.~Brouwer, V.~Elser and J.~P.~Sethna,
Sloppy-model universality class and the Vandermonde matrix,
{\it ?Phys. Rev. Lett.} \textbf{97} 150601, 2006.

\end{thebibliography}

\end{document}